Sam Shindich, [shindich@usc.edu](mailto:shindich@usc.edu)

I used the provided Fake News Detection dataset for my project. My preprocessing included combining both the real and fake articles into a single dataset with their respective truth labels, and dropped all columns except for the article contents (date, subject, and title). As BERT only takes in sentences, I felt that the simplest way to go about the problem would be to only analyze the actual articles; while the subjects seemed to look helpful, they were mismatched between the real and satire datasets, making their use very difficult. Finally, I dropped all duplicate articles that could have existed in the dataset. I used the BERT model, specifically for Sequence Classification, for my model. I started with 3 epochs and a batch size of 32 due to them being the recommendations by BERT, which ended up working out very well for me. Similarly, BERT recommends lower learning rates in the ranges of 2-5 \* 10^-5, so I used 5 \* 10^-5 and it worked out for me. I also chose the maximum sentence length of 512, due to me analyzing longer articles and wanting to utilize as much as I could from each article. My model showed negligible loss and 100 percent accuracy in both training and testing, meaning something is not fully working with my model (either optimizer isnâ€™t working properly or the way I calculate accuracy is somehow wrong); in any case, I will continue working on it to resolve this issue after the deadline. Either way, BERT should be a great choice for this dataset, as with over 27,000 to use for training, using an existing LLM for my task seemed to be the best way to go about my problem. While the premise of this project was to distinguish between real and fake news, I believe the particular dataset used is not perfect for this task, as a singular source determined that the articles were fake by flagging unreliable websites. In other words, labeling this data is difficult given that fact-checking is often influenced by biases and is not the easiest thing to do. In any case, after making my original model work, I would use a dataset with more relevant fake data and do my own cross-referenced research into which articles are real and fake.